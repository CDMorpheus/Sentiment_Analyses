{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "188a13ab",
   "metadata": {},
   "source": [
    "# Negative sentiment data\n",
    "I selected 100 words from collection of drug name/ slang. \\\n",
    "We have 100 drug related senteces. \\\n",
    "I augmented the 100 sentences by first translating them to hindi and then tralating these sentnces back to english, and applying sunonyms for 2 words in the final sentence.\\\n",
    "Once we have 100 drug words/slangs, and 200 drug related sentences, I replaced drug names in each sentence with all the drug name/ slangs in the dictionary.\\\n",
    "Thus I now have **10000 drug related negative sentiment sentences** for which model would know the importance of sentence as well as the occurance of word.\\\n",
    "In order to prevent our model from identifying the occurance of sentences as the source of negative sentence, I replaced all the drug names with nouns to give positive sentiment with same sentence in absence of drugs/slangs. This gives us **3500 positive sentiment sentences**.\n",
    "But we also have sentences where the sentiment is positive even if there is a mention of drug name/ slang. I took 25 such sentences and augmented them (as I did for 100 negative sentences) to get 50 sentences and fed in different drug names/slang. This gives us **5000 drug related positive sentiment sentences**.\\\n",
    "We take **1500 positive sentiment sentences** from amazon review dataset on amazon as is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2331be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dic= np.genfromtxt('./test.txt',dtype='str')\n",
    "len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8322f8e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['super acid',\n",
       " 'boomers',\n",
       " 'train',\n",
       " 'tussin',\n",
       " 'tweeker',\n",
       " 'god’s flesh',\n",
       " 'skippy',\n",
       " 'grass',\n",
       " 'crank',\n",
       " 'sugar',\n",
       " 'spice',\n",
       " 'downie',\n",
       " 'hug drug',\n",
       " 'duster',\n",
       " 'lady bub bles',\n",
       " 'ivory fresh',\n",
       " 'skittles',\n",
       " 'white dove',\n",
       " 'peanut',\n",
       " 'rainbows',\n",
       " 'gravel',\n",
       " 'stackers',\n",
       " 'musk',\n",
       " 'poppers',\n",
       " 'arctic blast',\n",
       " 'ts and rs',\n",
       " 'arnolds',\n",
       " 'chinese tobacco',\n",
       " 'ts and ritz',\n",
       " 'great  bear',\n",
       " 'bean',\n",
       " 'junk',\n",
       " 'shot',\n",
       " 'slick superspeed',\n",
       " 'hardball',\n",
       " 'goob',\n",
       " 'detroit pink',\n",
       " 'goodfellas',\n",
       " 'vitamin  d',\n",
       " 'happy dust',\n",
       " 'kit kat',\n",
       " 'snow leopard',\n",
       " 'fort dodge',\n",
       " 'bump',\n",
       " 'snow',\n",
       " 'coke',\n",
       " 'green',\n",
       " 'dihydrolone',\n",
       " 'tac',\n",
       " 'pumpers',\n",
       " 'magic mushrooms',\n",
       " 'stum bler',\n",
       " 'mama coca',\n",
       " 'caps',\n",
       " 'antifreeze',\n",
       " 'reynolds',\n",
       " 'herb',\n",
       " 'dabbing',\n",
       " 'twenty five',\n",
       " 'cadillac express',\n",
       " 'liquid x',\n",
       " 'skag',\n",
       " 'china town',\n",
       " 'china girl',\n",
       " 'crank',\n",
       " 'superman',\n",
       " 'little smoke',\n",
       " 'honey oil',\n",
       " 'beat',\n",
       " 'cat killer',\n",
       " 'jelly',\n",
       " 'belladonna',\n",
       " 'dabs',\n",
       " 'shrooms',\n",
       " 'snappers',\n",
       " 'liquid  ecstasy',\n",
       " 'go fast',\n",
       " 'trank',\n",
       " 'boom',\n",
       " 'lean',\n",
       " 'ecstasy',\n",
       " 'sweet stuff',\n",
       " 'peter pan',\n",
       " 'microdot',\n",
       " 'big o',\n",
       " 'blast',\n",
       " 'black whack',\n",
       " 'base',\n",
       " 'sherm',\n",
       " 'easy lay',\n",
       " 'blue kisses',\n",
       " 'peace',\n",
       " 'purple',\n",
       " 'pikachu',\n",
       " 'drugs',\n",
       " 'weed',\n",
       " 'love',\n",
       " 'meth',\n",
       " 'crystal',\n",
       " 'methamphetamine']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_list= np.random.choice(dic, size=93, replace=False) #choosing 200 words\n",
    "final_list= [(x.replace('-',' ')).lower() for x in list(dic_list)]\n",
    "final_dic= final_list + ['pikachu', 'drugs', 'weed', 'love','meth', 'crystal', 'methamphetamine'] #adding a few more words related to drugs to expand vocabulary\n",
    "(final_dic) #100 drug name/slang\n",
    "#alternatively, we could also make sure that all the drug names are present and then choose from the available drug slang."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b77117e",
   "metadata": {},
   "source": [
    "Augmenting dataset by changing sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d191a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing 100 drug related sentences:\n",
    "import pandas as pd\n",
    "df1 = pd.read_excel(r'.\\dialogou.xlsx')\n",
    "df2 = pd.read_excel(r'.\\dialogou_n.xlsx')\n",
    "df3 = pd.read_excel(r'.\\output.xlsx')\n",
    "df11=df1[df1['sentiment']==0][0].values[:]\n",
    "df22=df2[df2['sentiment']==0][0].values[:]\n",
    "df33=df3[df3['sentiment']==0][0].values[:]\n",
    "df= np.hstack((df11,df22[:-2],df33)) \n",
    "len(df) #100 drug related sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e203419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "#replacing all the drug words with 'meth' for simplicity and checking \n",
    "\n",
    "pattern = '\\\\b(' + '|'.join(list(dic) + ['pikachu', 'drugs', 'weed', 'love','meth', 'crystal', 'methamphetamine']) + ')\\\\b'\n",
    "replace_with = 'meth'\n",
    "\n",
    "for i, sentence in enumerate(df):\n",
    "    df[i] = re.sub(pattern, replace_with, sentence, flags=re.IGNORECASE)\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133e3aaa",
   "metadata": {},
   "source": [
    "Now augmenting these 100 sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f9bc86a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlob(\"Raw Meth\"), TextBlob(\"I met methy every night during the freshman year\"), TextBlob(\"Maybe we all need meth\"), TextBlob(\"We will never get this chance\"), TextBlob(\"Like meth needle and cold day\"), TextBlob(\"We have to spread meth\"), TextBlob(\"I am dreaming of a Meth Christmas. Today n or gunpowder receives any delivery tomoro and is not available till 10 00 pm\"), TextBlob(\"I saw the dirt that this is pure meth that you have received is the right product\"), TextBlob(\"All Meth going to California\"), TextBlob(\"Please give me some florida meth\"), TextBlob(\"I sometimes like the party very much, but that stuff is really addictive,\"), TextBlob(\"I find the person who was these meths and made a deal\"), TextBlob(\"A packet of Meth will be the perfect gift for my birthday\"), TextBlob(\"Look, there is some fame in that boat.\"), TextBlob(\"Keeping an eye on your blue meth\"), TextBlob(\"Smelly\"), TextBlob(\"Do you know that I am a friend who buys you rightly?\"), TextBlob(\"My customer will supply important information related to the organization, and the production of finger meth enterprise\"), TextBlob(\"Your Meth is really good,\"), TextBlob(\"Fresh Mensia 10 10 Hola back bits n draw and found that Morocco Puff Meth very good goods 8 10 still still still\"), TextBlob(\"Hello friend, do you smoke meths? If you do, tell me because I have some cheese here.\"), TextBlob(\"I was with a herd of meth heads\"), TextBlob(\"Sit-in-the-car business, but I know Meth Heads\"), TextBlob(\"I know a possible shipping point to get Meth\"), TextBlob(\"Raw without ripe Meth Shard, U can do it with how to get your order now\"), TextBlob(\"Felix is ​​going to transfer 70 tons of meth\"), TextBlob(\"You suck two suck on meth\"), TextBlob(\"Meth and hochers, my meth, are the key to success.\"), TextBlob(\"Im in stress can take meths to feel lighter\"), TextBlob(\"For all of you, ask more for the single Christmas don t worry 30 pounds for a meth cunt.\"), TextBlob(\"Meth's bags as a powerful smell, I weigh their value based on ideals of monetary gains.\"), TextBlob(\"I find this floor meth meth it feels fantastic\"), TextBlob(\"Smuggling people who use Meth\"), TextBlob(\"With all this Meth, we Meth for the whole month\"), TextBlob(\"Crazy Extra Weekend Weekend Amazing Meth and Bag Holler on MD\"), TextBlob(\"I hid my packet of Meth to avoid doubt\"), TextBlob(\"Some fresh Meth found a little rocky road there\"), TextBlob(\"I traded it for an ounce of Meth\"), TextBlob(\"Now, what you don't know about Meth Heads, they are unexpected\"), TextBlob(\"I felt it and enjoy it when I am on Meth\"), TextBlob(\"Bring meth to party tonight\"), TextBlob(\"I will get out of here\"), TextBlob(\"Do you want to join for meth\"), TextBlob(\"This is Meth Prime Quality, so do not stick your meth in it\"), TextBlob(\"It was definitely a much better size than our old meth\"), TextBlob(\"It was easy to go in, because no one was worried about Meth\"), TextBlob(\"Will choose Meth above sleep on sex on life.\"), TextBlob(\"You are returning to the meth business again\"), TextBlob(\"Let us sniff with a bovie knife\"), TextBlob(\"This idea was to replace gas from meth\"), TextBlob(\"I can't leave Meth. Meth is life.\"), TextBlob(\"Me and my boyfriend became high on meth\"), TextBlob(\"Drug chemist brings his meth product to Colombian smugglers\"), TextBlob(\"Cartel Meth taking shots\"), TextBlob(\"I have the will to fight and feel good when I take Meth\"), TextBlob(\"So you are making a Meth more expensive, why don't you do me a favor\"), TextBlob(\"It may be unrelated, but blue meth is making a little back\"), TextBlob(\"The chemist was in possession of Meth\"), TextBlob(\"You must be looking at Angels at any time, which never stops Manz on the New Year's day on the eve of Exams.\"), TextBlob(\"How much meth you can distribute at once\"), TextBlob(\"IVE received some information where we can buy meth\"), TextBlob(\"Very good meth for money on this\"), TextBlob(\"He was stealing with him and selling his meth\"), TextBlob(\"Call me good meth for money\"), TextBlob(\"I did not find any meth, it will take away from the money that you are outstanding and it is a little cheap that you soon speak man\"), TextBlob(\"Only the reason I hope in Meth because it has gone for a long time\"), TextBlob(\"Stress is dangerous how Meth is working\"), TextBlob(\"I got fresh meth and it was fun\"), TextBlob(\"Hi, can I get some meth today in any chance?\"), TextBlob(\"We just came to meth\"), TextBlob(\"Yeh mate thats cold meth is very good, although direct mensia from the dam\"), TextBlob(\"Ho ho ho ho\"), TextBlob(\"Meth Ship did very well for us\"), TextBlob(\"Get the Gold Fort Math and this fort is brother\"), TextBlob(\"My dealer has a better offer for Meth\"), TextBlob(\"American American was doing a few hundred tons of meths every year to satisfy the nose\"), TextBlob(\"Rolls Royce Meth Tab 210 each if you are referred to by a meth\"), TextBlob(\"Did you try that Meth cheese? Oh, you are going to enjoy it\"), TextBlob(\"Roads are full of drug dealers and I will buy something\"), TextBlob(\"Hey as we all know our Valentines Day tomorrow, so from today, I am playing my part showing meths with a wonderful deal.\"), TextBlob(\"Special updated Lady Kate is back to provide her services\"), TextBlob(\"I know that Meth is bad but it looks great\"), TextBlob(\"I get a hit of that meth\"), TextBlob(\"Are you an expert in Meth, can you teach me that I am not wasting my time.\"), TextBlob(\"This is a meth grade. I mean, you have found Jesus, you have two inches, three inches long, this pure meth.\"), TextBlob(\"Meth hijacks the pleasure centers in the brain\"), TextBlob(\"Do you want to cook meths? Yes, I know that a friend who wants to sell that bomb would mean.\"), TextBlob(\"Want to get some enthusiasm in my life with Meth.\"), TextBlob(\"Do you know that Meth has more demand than anywhere else in Europe\"), TextBlob(\"I'm here to buy meth\"), TextBlob(\"It is an ideal place to gather and fastens all night\"), TextBlob(\"I had something else to experience Meth, I can't describe in words\"), TextBlob(\"Children distribute meth package to me\"), TextBlob(\"We had one of the best times with Meth\"), TextBlob(\"A meth and im\"), TextBlob(\"So much weight it is pulling me down, gives me relief from this aromatic meth bounty delivery 6pm\"), TextBlob(\"These are my good clothes that I can't go home like a meth\"), TextBlob(\"You act like a gorilla after meth\"), TextBlob(\"Nothing stops this meth\"), TextBlob(\"This is a party that enjoys it with Meth\")]\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "\n",
    "dataset = (df)\n",
    "\n",
    "# Translation  to augmented dataset\n",
    "synonym_dataset = []\n",
    "for sentence in dataset:\n",
    "    \n",
    "    #english to hindi\n",
    "    blob = TextBlob(sentence).translate(from_lang='en', to='hi')\n",
    "    translated= blob.translate(from_lang='hi', to='en')\n",
    "    \n",
    "    synonym_dataset.append(translated.strip())\n",
    "\n",
    "# Remove duplicates from augmented dataset\n",
    "final_dataset = list(set(synonym_dataset))\n",
    "\n",
    "# Print final dataset\n",
    "print(final_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee585ea",
   "metadata": {},
   "source": [
    "Observing this, it is apparent that a few sentences have been changed without loosing the meaning but for a few, textblob gave poor results, so I chose to do only synonyms augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "969566c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to nltk_data/...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to nltk_data/...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ddave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yeh mate thats cool the meth is so nice though mate amnesia heterosexual from the dam', 'raw uracilncooked meth shard so uracil can do with it how uracil please limited suracilpplies get youracilr orders in now', 'with all this meth, we meth for the whole month', 'do you want to articulation for a meth', 'you ll be visual_perception angels in no time dial manz in drops xmas eve to new old_age day never stopping.', 'drug chemist brings his meth product to colombian smuggler', 'félix is about to move 70 tons of meth through juárez', 'good meth for money call me', 'all the meth is going to california', 'i ve get no meth on me will take it off the money you owe me and do it a bit cheap for you talk soon dude', 'bring meth to the party tonight', 'hey as we all know its valentines day tomorrow so from today im playing my part screening meth with an amazing deal', 'we will never get this chance again let United_States do meth', 'i got fresh meth and it was fun', \"it's a party semen on enjoy it with meth\", 'he was stealing from him all along and selling his meth', 'please give me some florida meth', 'you two sucking at peddling meth', 'nothing stops this meth', 'i know a possible transportation points for getting meth', \"it's a perfective spot for a gather and do meth all night\", 'are you an expert on meth, can you Teach me i Don’t like my time being wasted.', \"i'm here to buy meth\", \"did you try that meth thing? Ohio, you're gonna enjoy this\", 'so much weight it dragging me down relieve me of this fragrant meth bounty delivery 6pm 1 am', \"stress is dangerous how's the meth working out\", \"you're right back in the meth business again\", 'my trader has a better offer for the meth', 'smelly meth big bags pineapple three four two dont sleep best in four time hollet inabit', 'rolls royce meth tabs 210 each if you are referred by a meth', \"these are my good clothes i calciumn't go home smelling like a meth\", 'sit-in-the-car business, but i know meth head', 'fresh amnesia ten ten holla back bits n draws and got that moroccan puff meth very good stuff eight ten still', 'one meth and im make', 'you act like gorillas after meth', 'get sum killa meth of gold and it s killa bro', 'been keeping an eye on that blue meth of yours', 'want to get a little excitement in my life with meth.', 'i hid my package of meth to avoid suspicion', 'how much meth can you deliver at once', \"you know i'm a dude that bargain meth off you right?\", 'im in stress, might return meth to feel igniter', 'hello mate, do you smoke meth? if you do then Lashkar-e-Taiba me know because i have some absolutely banging cheese here.', 'Lashkar-e-Taiba us snicker meth off a bowie knife', \"i can't quit meth. meth are life.\", 'who smuggled meth to are the ones who use the meth', 'americans were doing a couple hundred tons of meth every year to satisfy american nose', \"my client will supply vital information relating to the organization, and production of fring's meth enterprise\", \"now, what you may not know about meth heads they're kind of unpredictable\", 'meth and Hooker, my meth, are the keys to success.', 'maybe all we need is the meth', 'i meth this floor meth it feel antic', 'i meth it and enjoy when i am on meth', \"i smoke meth a lot sometimes i party but that material's truly addictive,\", 'i m dream of a meth christmas today nitrogen get your or ammunitrogenitionitrogen nitrogeno deliveries tomoro anitrogend nitrogenot available till 10 00 pm', 'getting in was easy, because nobody worried about meth', \"did you know there's a higher demand for meth there than anywhere else in europe\", 'kids deliver packages of meth to me', 'i will be out here smoking meth', 'this may be unrelated, but the blue meth is making a spot of a back', 'experience meth was something else, i cannot describe in words', 'the cartel is pickings meth shots', 'me and my boyfriend got high on meth together', 'chemist was in possession of meth', 'let me get a hit of that meth', 'got some fresh meth there little rocky road bonanzas it', 'meth needle and chill kind of day', 'the bags of meth smell potent as i weigh up their value establish on the ideals of monetary gain', 'we had one of the best times with meth', \"only reason i have a hope in meth is because it's long travel\", 'we just get meth come over', 'special update lady ket is back to supply her services of meth', 'hi any opportunity i could get some meth today?', 'very good meth for money on this one', 'the idea was to replace the gas with meth and send it', 'the meth ship did pretty good for us', 'a package of meth would be perfect gift for my birthday', 'i did meth everynight during freshman year', \"you want to cook meth?  yeah i know a dude who privation to sell that'd be the bomb i mean.\", \"that meth is prime quality so don't go sticking your meth in it\", \"so you're the one making meth more expensive come on, why don't you do me a favor\", 'i trade it for an ounce of meth', 'this is meth grade i mean, you got jesus, you got crystals in here two inch, three inch hanker this is pure meth', 'i was with  a bunch of meth heads', 'raw meth that will meth sadams sandles off', 'would choose meth over sleep, over sex over life itself.', 'meth hijacks the pleasure centers in the brain', 'ive got some information where we can bargain meth', 'i know meth is bad but it feel so good', 'i find the person that had these meth and made a deal', \"i proverb this shit there it's pure meth what you got thereis the perfective product\", 'we have to spread meth not hate anyway meth is about lets enjoy responsibly', 'for all you singleton this christmas don t worry 30 pound a meth cunt ask for more under the misletoe holmium holmium holmium', 'your meth is truly good,', 'crazy extras on md is weekend amazing meth and bag also bellow', 'holmium holmium holmium meth and go merry christmas from the best dealer for meth uracil no', 'it was certainly in a lot better shape than our old meth', 'expression there is some meth in that boat.', 'the street are full of drug trader and i will buy something', 'i have the will to fight all and feel good when i return meth']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet', \"nltk_data/\")\n",
    "\n",
    "nltk.download('omw-1.4', \"nltk_data/\")\n",
    "\n",
    "nltk.data.path.append('nltk_data/')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# function to get list of words after removing stop words\n",
    "def remove_stopwords(text):\n",
    "    #\"nlp\" Object is used to create documents with linguistic annotations.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    filtered_sentence = [word for word in text.split() if word.lower() not in stop_words]\n",
    "    filtered_sentence = ' '.join(filtered_sentence)\n",
    "    #print(token_list)\n",
    "    #print(filtered_sentence)\n",
    "    return filtered_sentence\n",
    "\n",
    "# random insert function\n",
    "def random_insert(sentence):\n",
    "    # n is the number of words to replace in sentence\n",
    "\n",
    "    # get the list of words excluding stop words\n",
    "    sentence_ = remove_stopwords(sentence)\n",
    "\n",
    "    words = nltk.word_tokenize(sentence_)\n",
    "    new_sentence = ''\n",
    "    \n",
    "    repeated_words = [] \n",
    "    replace_words = []\n",
    "    # number of words changed depends on the number of words in sentence 40% are changed\n",
    "    n= int(len(words)*0.4)\n",
    "    for i in range (n):\n",
    "    # choose a random word\n",
    "        word = random.choice(words)\n",
    "\n",
    "        # if word has already been choosen before move to next word\n",
    "        if word in repeated_words:\n",
    "            continue\n",
    "        if word ==('meth'):\n",
    "            continue\n",
    "            \n",
    "        repeated_words.append (word)\n",
    "\n",
    "        # get closest synonym of the word\n",
    "        new_synonym = wordnet.synsets(word)\n",
    "        if new_synonym:\n",
    "            new_word = new_synonym[0].lemmas()[0].name()\n",
    "            if new_word != word:\n",
    "                \n",
    "                # maintain a list of words and their synonyms to be replaced\n",
    "                replace_words.append (new_word)\n",
    "\n",
    "                # replace the synonym with the word in the sentence\n",
    "                \n",
    "                sentence = sentence.replace(word, new_word)\n",
    "    sentence= ''.join(sentence)\n",
    "    replace_words= ' '.join(replace_words)\n",
    "    return sentence, replace_words\n",
    "\n",
    "synonym_dataset = []\n",
    "word_list=[]\n",
    "\n",
    "for sentence in df:\n",
    "    translated, words_rep= random_insert(sentence.lower())\n",
    "    \n",
    "    word_list.append(words_rep.strip())\n",
    "    synonym_dataset.append(translated.strip())\n",
    "\n",
    "# Remove duplicates from augmented dataset\n",
    "final_dataset = list(set(synonym_dataset))\n",
    "\n",
    "# Print final dataset\n",
    "print(final_dataset)\n",
    "negative_200= np.hstack((final_dataset,df)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "526ef831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ddf285",
   "metadata": {},
   "source": [
    "similarly for 25 positive sentiment drug related sentences:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90afbc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "dfp1=df1[df1['change']==1][0].values[:]\n",
    "dfp2=df2[df2['change']==1][0].values[:]\n",
    "\n",
    "dfp= np.hstack((dfp1,dfp2)) \n",
    "# converting all drug names to meth for simplicity\n",
    "pattern = '\\\\b(' + '|'.join(list(dic) + ['pikachu', 'drugs', 'weed', 'love','meth', 'crystal', 'methamphetamine']) + ')\\\\b'\n",
    "replace_with = 'meth'\n",
    "\n",
    "for i, sentence in enumerate(dfp):\n",
    "    dfp[i] = re.sub(pattern, replace_with, sentence, flags=re.IGNORECASE)\n",
    "\n",
    "print(len(dfp))\n",
    "# augmentation\n",
    "synonym_datasetp = []\n",
    "word_listp=[]\n",
    "\n",
    "for sentence in dfp:\n",
    "    translated, words_rep= random_insert(sentence.lower())\n",
    "    # number of words changed depends on the number of words in sentence 40% are changed\n",
    "    word_listp.append(words_rep.strip())\n",
    "    synonym_datasetp.append(translated.strip())\n",
    "\n",
    "# Remove duplicates from augmented dataset\n",
    "final_datasetp = list(set(synonym_datasetp))\n",
    "positve_50_drugs= np.hstack((final_datasetp,dfp)) \n",
    "print(len(positve_50_drugs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de47337f",
   "metadata": {},
   "source": [
    "Importing list of nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f94dfcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nouns = (pd.read_excel(r'.\\nouns.xlsx') )#Name of indian dishes \n",
    "df_nouns=list(df_nouns['Nouns'].values[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bca7e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500 10000\n"
     ]
    }
   ],
   "source": [
    "n_o=negative_200[:100]\n",
    "n_e=negative_200[100:]\n",
    "df_o=df[:100]\n",
    "df_e=df[100:] \n",
    "\n",
    "pattern = '\\\\b(' + '|'.join(list(dic) + ['pikachu', 'drugs', 'weed', 'love','meth', 'crystal', 'methamphetamine']) + ')\\\\b'\n",
    "\n",
    "negative_sentiment=[]\n",
    "\n",
    "\n",
    "dfp_1=df1[df1['change']==0][0].values[:]\n",
    "dfp_2=df1[df1['change']==0][0].values[:]\n",
    "\n",
    "\n",
    "positive_sentiment= list(np.hstack((dfp_1,dfp_2))) \n",
    "\n",
    "temp=[]\n",
    "for i in range (100):\n",
    "    replace_with = final_dic[i]\n",
    "    replace_with_noun = df_nouns[i]\n",
    "    for i, sentence in enumerate(positve_50_drugs):\n",
    "            positive_sentiment.append(re.sub(pattern, replace_with, sentence, flags=re.IGNORECASE))\n",
    "    \n",
    "    if i%2==0:\n",
    "        for i, sentence in enumerate(n_e):\n",
    "            negative_sentiment.append(re.sub(pattern, replace_with, sentence, flags=re.IGNORECASE))\n",
    "        for i, sentence in enumerate(df_e):\n",
    "            temp.append(re.sub(pattern, replace_with_noun, sentence, flags=re.IGNORECASE))\n",
    "    else: \n",
    "        for i, sentence in enumerate(n_o):\n",
    "            negative_sentiment.append(re.sub(pattern, replace_with, sentence, flags=re.IGNORECASE))\n",
    "        for i, sentence in enumerate(df_o):\n",
    "            temp.append(re.sub(pattern, replace_with_noun, sentence, flags=re.IGNORECASE))\n",
    "\n",
    "selected_sentences = random.sample(temp, k=int(3500-len(positive_sentiment)+5000))\n",
    "\n",
    "positive_sentiment=np.hstack((selected_sentences,positive_sentiment))\n",
    "print(len(positive_sentiment), len(negative_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff5a8add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.savetxt('negative_sentiment.txt', negative_sentiment, fmt=\"%s\")\n",
    "(pd.DataFrame(negative_sentiment)).to_excel(\"negative_sentiment.xlsx\")\n",
    "len(negative_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae4274",
   "metadata": {},
   "source": [
    "Rest 1500 positive sentiment sentences comes from the amazon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46e47899",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list((pd.read_excel(r'.\\amazon.xlsx') )[0].values[:])\n",
    "amazon=[]\n",
    "for sentence in sentences:\n",
    "    summary_sentences = sentence[:80]\n",
    "    amazon.append(summary_sentences.replace(\".\", \"\"))\n",
    "\n",
    "positive_sentiment=np.hstack((amazon[:-88],positive_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d7004a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac59b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('positive_sentiment.txt', positive_sentiment, fmt=\"%s\")\n",
    "(pd.DataFrame(positive_sentiment)).to_excel(\"positive_sentiment.xlsx\") \n",
    "np.savetxt('drug vocab for training.txt', final_dic, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d75aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
