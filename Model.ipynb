{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3b4569",
   "metadata": {},
   "source": [
    "the following code is highly inspired from https://skimai.com/fine-tuning-bert-for-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1fe9711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fc0b6dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>yeh mate thats cool the super acid is so nice ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>raw uracilncooked super acid shard so uracil c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>with all this super acid, we super acid for th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>do you want to articulation for a super acid</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>you ll be visual_perception angels in no time ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>9995</td>\n",
       "      <td>Just say no to methamphetamine and yes to life</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>9996</td>\n",
       "      <td>suit up against a new enemy methamphetamine th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>9997</td>\n",
       "      <td>So no to methamphetamine because it is killing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>9998</td>\n",
       "      <td>when it comes to methamphetamine and alcohol  ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>9999</td>\n",
       "      <td>I do not smuggle methamphetamine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                                  0  label\n",
       "0               0  yeh mate thats cool the super acid is so nice ...      0\n",
       "1               1  raw uracilncooked super acid shard so uracil c...      0\n",
       "2               2  with all this super acid, we super acid for th...      0\n",
       "3               3       do you want to articulation for a super acid      0\n",
       "4               4  you ll be visual_perception angels in no time ...      0\n",
       "...           ...                                                ...    ...\n",
       "19995        9995     Just say no to methamphetamine and yes to life      1\n",
       "19996        9996  suit up against a new enemy methamphetamine th...      1\n",
       "19997        9997  So no to methamphetamine because it is killing...      1\n",
       "19998        9998  when it comes to methamphetamine and alcohol  ...      1\n",
       "19999        9999                   I do not smuggle methamphetamine      1\n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative = pd.read_excel(r'.\\negative_sentiment.xlsx')\n",
    "negative['label'] = 0\n",
    "positive = pd.read_excel(r'.\\positive_sentiment.xlsx')\n",
    "positive['label'] = 1\n",
    "\n",
    "# Concatenate complaining and non-complaining data\n",
    "data = pd.concat([negative, positive], axis=0).reset_index(drop=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "61d194ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_0 = negative[0].values\n",
    "y_0 = negative['label'].values\n",
    "\n",
    "X_1 = positive[0].values\n",
    "y_1 = positive['label'].values\n",
    "\n",
    "X0_train, X0_val, y0_train, y0_val =\\\n",
    "    train_test_split(X_0, y_0, test_size=0.3, random_state=2020)\n",
    "X1_train, X1_val, y1_train, y1_val =\\\n",
    "    train_test_split(X_1, y_1, test_size=0.3, random_state=2020)\n",
    "X_train = np.hstack((X0_train, X1_train))\n",
    "y_train = np.hstack((y0_train, y1_train))\n",
    "X_val = np.hstack((X0_val, X1_val))\n",
    "y_val = np.hstack((y0_val, y1_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5f774f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "efe5dd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ddave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Uncomment to download \"stopwords\"\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_preprocessing(s):\n",
    "    \"\"\"\n",
    "    - Lowercase the sentence\n",
    "    - Change \"'t\" to \"not\"\n",
    "    - Remove \"@name\"\n",
    "    - Isolate and remove punctuations except \"?\"\n",
    "    - Remove other special characters\n",
    "    - Remove stop words except \"not\" and \"can\"\n",
    "    - Remove trailing whitespace\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    # Change 't to 'not'\n",
    "    s = re.sub(r\"\\'t\", \" not\", s)\n",
    "    # Remove @name\n",
    "    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n",
    "    # Isolate and remove punctuations except '?'\n",
    "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
    "    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
    "    # Remove some special characters\n",
    "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
    "    # Remove stopwords except 'not' and 'can'\n",
    "    s = \" \".join([word for word in s.split()\n",
    "                  if word not in stopwords.words('english')\n",
    "                  or word in ['not', 'can']])\n",
    "    # Remove trailing whitespace\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5969c25b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6ea08e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  167\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(sentence) for sentence in data[0].values])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "671dda09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\ddave\\anaconda3\\envs\\pytorch_\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  yeh mate thats cool the super acid is so nice though mate amnesia heterosexual from the dam\n",
      "Token IDs:  [101, 6300, 2232, 6775, 2008, 2015, 4658, 3565, 5648, 3835, 2295, 6775, 29222, 28229, 5477, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = max_len\n",
    "\n",
    "# Print sentence 0 and its encoded token ids\n",
    "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
    "print('Original: ', X[0])\n",
    "print('Token IDs: ', token_ids)\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "aa4a0430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e4b04680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "35037ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=2):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=1.1e-4,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c031af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_per_itr=[]\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=2, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "                torch.save(model, 'model_updated.pth') #saving model\n",
    "                loss_per_itr.append(batch_loss / batch_counts)\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3acd48",
   "metadata": {},
   "source": [
    "(Do not run this cell while testing- this is the training function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1af82b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\ddave\\anaconda3\\envs\\pytorch_\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.623472   |     -      |     -     |  922.71  \n",
      "   1    |   40    |   0.369023   |     -      |     -     |  943.65  \n",
      "   1    |   60    |   0.283889   |     -      |     -     |  1052.66 \n",
      "   1    |   80    |   0.148829   |     -      |     -     |  1093.88 \n",
      "   1    |   100   |   0.080028   |     -      |     -     |  817.38  \n",
      "   1    |   120   |   0.060123   |     -      |     -     |  735.31  \n",
      "   1    |   140   |   0.018878   |     -      |     -     |  730.66  \n",
      "   1    |   160   |   0.014458   |     -      |     -     |  737.31  \n",
      "   1    |   180   |   0.037358   |     -      |     -     |  732.45  \n",
      "   1    |   200   |   0.040755   |     -      |     -     |  736.11  \n",
      "   1    |   220   |   0.043269   |     -      |     -     |  742.13  \n",
      "   1    |   240   |   0.056732   |     -      |     -     |  866.74  \n",
      "   1    |   260   |   0.058927   |     -      |     -     |  824.84  \n",
      "   1    |   280   |   0.027674   |     -      |     -     |  681.21  \n",
      "   1    |   300   |   0.056485   |     -      |     -     |  661.41  \n",
      "   1    |   320   |   0.045806   |     -      |     -     |  688.88  \n",
      "   1    |   340   |   0.010309   |     -      |     -     |  822.37  \n",
      "   1    |   360   |   0.026241   |     -      |     -     |  826.86  \n",
      "   1    |   380   |   0.005094   |     -      |     -     |  827.95  \n",
      "   1    |   400   |   0.001148   |     -      |     -     |  817.38  \n",
      "   1    |   420   |   0.022611   |     -      |     -     |  815.04  \n",
      "   1    |   437   |   0.001464   |     -      |     -     |  669.90  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.094225   |  0.006342  |   99.90   | 20958.25 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.010366   |     -      |     -     |  850.87  \n",
      "   2    |   40    |   0.001056   |     -      |     -     |  800.14  \n",
      "   2    |   60    |   0.018456   |     -      |     -     |  670.42  \n",
      "   2    |   80    |   0.009795   |     -      |     -     |  716.28  \n",
      "   2    |   100   |   0.009995   |     -      |     -     |  761.77  \n",
      "   2    |   120   |   0.007963   |     -      |     -     |  764.10  \n",
      "   2    |   140   |   0.009744   |     -      |     -     |  749.85  \n",
      "   2    |   160   |   0.005630   |     -      |     -     |  750.60  \n",
      "   2    |   180   |   0.000895   |     -      |     -     |  752.31  \n",
      "   2    |   200   |   0.000575   |     -      |     -     |  754.68  \n",
      "   2    |   220   |   0.011068   |     -      |     -     |  751.94  \n",
      "   2    |   240   |   0.000623   |     -      |     -     |  676.69  \n",
      "   2    |   260   |   0.000536   |     -      |     -     |  597.14  \n",
      "   2    |   280   |   0.000534   |     -      |     -     |  579.61  \n",
      "   2    |   300   |   0.000515   |     -      |     -     |  573.42  \n",
      "   2    |   320   |   0.005866   |     -      |     -     |  576.83  \n",
      "   2    |   340   |   0.000522   |     -      |     -     |  579.47  \n",
      "   2    |   360   |   0.000441   |     -      |     -     |  572.73  \n",
      "   2    |   380   |   0.000430   |     -      |     -     |  571.47  \n",
      "   2    |   400   |   0.000425   |     -      |     -     |  576.30  \n",
      "   2    |   420   |   0.000420   |     -      |     -     |  592.33  \n",
      "   2    |   437   |   0.000446   |     -      |     -     |  493.63  \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.004418   |  0.002006  |   99.95   | 17143.02 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b89b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save('./model') #saving model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c875f8",
   "metadata": {},
   "source": [
    "loading the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d57d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= torch.load('.\\model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3bd1381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "59eef5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = bert_predict(bert_classifier, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "324d75c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "afbecfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.95%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "   \n",
    "    preds = probs[:, 1]\n",
    "    \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    \n",
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9f8ef99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x= np.linspace(0,440,44)\n",
    "plt.plot(x, loss_per_itr)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('batch training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ad995165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences predicted positive:  3000\n",
      "positive sentences as predicted by model: \n",
      " ['when it comes to blue kisses and alcohol  just say no'\n",
      " 'no bomb, no beer, no poppers you smoke it up on your own time'\n",
      " 'I recently received my Amazon Fire TV after months of researching which streamin'\n",
      " 'my relation with slick superspeed is in history book now'\n",
      " \"Stress is dangerous How's the Batata Saung working out\"\n",
      " 'our organization, within six calendar_month,will be out of the happy dust business'\n",
      " ' Good Locha for money Call me ' 'coke are not good for health'\n",
      " ' The bags of Bajri no rotlo smell potent as i weigh up their value based on the ideals of monetary gain'\n",
      " ' So much weight it dragging me down Relieve me of this fragrant Amti bounty Delivery 6pm 1 am '] \n",
      "negative sentences as predicted by model: \n",
      " ['been keeping an eye on that blue snow of yours'\n",
      " 'rolls royce vitamin  d tabs 210 each if you are referred by a vitamin  d'\n",
      " 'you act like gorillas after arctic blast'\n",
      " 'snow and Hooker, my snow, are the keys to success.'\n",
      " 'Lashkar-e-Taiba us snicker super acid off a bowie knife'\n",
      " 'the cartel is pickings methamphetamine shots'\n",
      " 'i ve get no coke on me will take it off the money you owe me and do it a bit cheap for you talk soon dude'\n",
      " \"it's a party semen on enjoy it with stum bler\"\n",
      " 'the idea was to replace the gas with pikachu and send it'\n",
      " 'very good shot for money on this one']\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.95\n",
    "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
    "\n",
    "# Number of tweets predicted non-negative\n",
    "print(\"Number of sentences predicted positive: \", preds.sum())\n",
    "output_positive = X_val[preds==1]\n",
    "output_negative = X_val[preds==0]\n",
    "print('positive sentences as predicted by model: \\n', output_positive[-10:],'\\nnegative sentences as predicted by model: \\n',output_negative[:10]  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2ca27",
   "metadata": {},
   "source": [
    "Optionally, the method showed on the website: https://medium.com/@lorenagongang/sentiment-analysis-on-streaming-twitter-data-using-kafka-spark-structured-streaming-python-part-b27aecca697a could be adopted to create pipelines of data and perform sentiment analyses on streaming data from other APIs. This could be an extension for future work which I beleive would better serve the use case for your business, since you could monitor chats in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1429235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
